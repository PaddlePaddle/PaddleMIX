## ğŸ“¦ PaddleMIXå·¥å…·ç®±ä»‹ç» ğŸ“¦
PaddleMIXå·¥å…·ç®±ç§‰æ‰¿äº†é£æ¡¨å¥—ä»¶ä¸€ç«™å¼ä½“éªŒã€æ€§èƒ½æè‡´ã€ç”Ÿæ€å…¼å®¹çš„è®¾è®¡ç†å¿µï¼Œæ—¨åœ¨æä¾›ä¸šç•Œä¸»æµè·¨æ¨¡æ€å¤§æ¨¡å‹å…¨æµç¨‹ç»Ÿä¸€å·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…ä½æˆæœ¬ã€ä½é—¨æ§›ã€å¿«é€Ÿå®ç°è·¨æ¨¡æ€å¤§æ¨¡å‹å®šåˆ¶åŒ–ã€‚

[[English](README_en.md)]

##  ğŸ› ï¸ Unified Fine-tuning Tool for Multimodal Understanding ğŸ› ï¸

| Model |  SFT | LoRA | Deploy | NPU training |
| --- |  --- | --- | --- | --- | 
| [YOLO-World](./YOLO-World/) | âŒ  | âŒ  | âŒ | âŒ |
| [audioldm2](./audioldm2/) | âŒ | âŒ | âŒ | âŒ |
| [blip2](./blip2/) | âœ…  | âœ… |  âŒ | âŒ |
| [clip](./clip) |âŒ | âŒ | âŒ | âŒ |
| [coca](./coca/) |  âŒ | âŒ | âŒ | âŒ |
| [CogVLM && CogAgent](./cogvlm/) |âŒ | âŒ | âŒ | âŒ |
| [eva02](./eva02/)|   âœ…  |  âŒ   | âŒ   | âŒ |
| [evaclip](./evaclip/) | âŒ | âŒ |  âŒ | âŒ |
| [groundingdino](./groundingdino/) |  ğŸš§   | âŒ  | âœ…  | âŒ |
| [imagebind](./imagebind/) |  âŒ  | âŒ | âŒ | âŒ |
| [InternLM-XComposer2](./internlm_xcomposer2/) | âœ… | âŒ | âŒ | âŒ |
| [Internvl2](./internvl2/)| âœ… | âŒ | âŒ | âœ… |
| [llava](./llava/)  | âœ…  | âœ…  | ğŸš§  | âœ… |
| [llava-next](./llava_next_interleave/) | âŒ | âŒ | âŒ | âŒ |
| [minigpt4](./minigpt4) | âœ…   |  âŒ  | âœ…  | âŒ |
| [minimonkey](./minimonkey/) | âœ… | âŒ | âŒ | âŒ |
| [qwen2_vl](./qwen2_vl/)| âœ… | âŒ | âŒ | âŒ |
| [qwen_vl](./qwen_vl/)  | âœ…  | âœ…  | âœ…  | âŒ |
| [sam](./sam/) | âŒ | âŒ | âœ…  | âŒ |
| [visualglm](./visualglm/) | âœ… | âœ… | âŒ | âŒ |

* âœ…: Supported
* ğŸš§: In Progress
* âŒ: Not Supported

æ³¨æ„ï¼š
1. å¼€å§‹å‰è¯·å…ˆæŒ‰ç…§[ç¯å¢ƒä¾èµ–](../../README.md#å®‰è£…)å®‰è£…ç¯å¢ƒï¼Œä¸åŒæ¨¡å‹è¯·å‚è€ƒ [examples](../examples/README.md) ä¸‹å¯¹åº”çš„æ¨¡å‹ç›®å½•å®‰è£…ä¾èµ–ï¼›
2. å½“å‰**tools**ç»Ÿä¸€æ¥å£åªæ”¯æŒéƒ¨åˆ†æ¨¡å‹çš„ç²¾è°ƒèƒ½åŠ›ï¼Œå…¶ä»–æ¨¡å‹åŠå…¶ä»–èƒ½åŠ›åç»­é™†ç»­ä¸Šçº¿ã€‚


##  ğŸš€ å¿«é€Ÿå¼€å§‹ ğŸš€

### 1. ç²¾è°ƒ
PaddleMIX ç²¾è°ƒæ”¯æŒå¤šä¸ªä¸»æµè·¨æ¨¡æ€å¤§æ¨¡å‹çš„SFTã€LoRAç­‰ç²¾è°ƒç­–ç•¥ï¼Œæä¾›ç»Ÿä¸€ã€é«˜æ•ˆç²¾è°ƒæ–¹æ¡ˆï¼š
- **ç»Ÿä¸€è®­ç»ƒå…¥å£** PaddleMIX ç²¾è°ƒæ–¹æ¡ˆå¯é€‚é…ä¸šç•Œä¸»æµè·¨æ¨¡æ€å¤§æ¨¡å‹ï¼Œç”¨æˆ·åªéœ€ä¿®æ”¹[config](../config/) é…ç½®æ–‡ä»¶ï¼Œå³èƒ½åœ¨å•å¡æˆ–å¤šå¡è¿›è¡Œå¤šç§å¤§æ¨¡å‹ç²¾è°ƒï¼›
- **å¤šæ•°æ®é›†å’Œæ··åˆæ•°æ®é›†** æ”¯æŒå¤šç§æ•°æ®é›†å’Œæ··åˆæ•°æ®é›†åŒæ—¶ç²¾è°ƒï¼ŒåŒ…æ‹¬ï¼šVQAã€Captionã€Chatmlç­‰æ•°æ®é›†æ··åˆä½¿ç”¨ï¼›
- **å¼ºå¤§æ•°æ®æµå’Œåˆ†å¸ƒå¼ç­–ç•¥** MIXTokenç­–ç•¥æœ‰æ•ˆå¢åŠ æ•°æ®ååé‡ï¼Œå¤§å¹…åº¦æé«˜æ¨¡å‹è®­ç»ƒæ•ˆç‡ã€‚è‡ªé€‚åº”Trainerå’Œå®šåˆ¶åŒ–Trainerçµæ´»é…ç½®ï¼Œæ— ç¼é“¾æ¥é£æ¡¨åˆ†å¸ƒå¼å¹¶è¡Œç­–ç•¥ï¼Œå¤§å¹…é™ä½å¤§æ¨¡å‹ç²¾è°ƒç¡¬ä»¶é—¨æ§›ã€‚


**æ•°æ®å‡†å¤‡**ï¼š

æˆ‘ä»¬æ”¯æŒå¤šæ•°æ®é›†ã€æ··åˆæ•°æ®é›†åŒæ—¶ç”¨äºç²¾è°ƒï¼Œé€šè¿‡**MixDataset**ç»Ÿä¸€è°ƒåº¦ï¼Œç”¨æˆ·åªéœ€åœ¨é…ç½®æ–‡ä»¶æŒ‡å®š [dataset](../datasets/) æ”¯æŒçš„æ•°æ®é›†ç»„æˆé›†åˆï¼Œå³å¯ä½¿ç”¨ã€‚å¦‚ï¼š

```
# config.json
...

"dataset": {
      "train":[
        {"name": "chatml_dataset", "data_files": "train.json"ï¼Œ"chat_template":"chat_template.json"},
        {"name": "coco_caption", "data_files": "train.json"},
        ......],
      "eval": [
        {"name": "chatml_dataset", "data_files": "val.json"ï¼Œ"chat_template":"chat_template.json"},
        {"name": "coco_caption", "data_files": "val.json"},
        ......],
    },
....

```

è€Œå¯¹äºæ¯ä¸ªå­æ•°æ®é›†ï¼Œå¦‚ä¸Šè¿°çš„ coco_caption æ•°æ®æ ¼å¼ï¼Œå¯å‚è€ƒ[examples](../examples/) ä¸‹å¯¹åº”çš„æ¨¡å‹ç›®å½•é‡Œé¢çš„æ–‡æ¡£ä»‹ç»ã€‚

åŒæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡ chatml_dataset æ•°æ®é›†æ”¯æŒä¸»æµçš„LLMå¯¹è¯è‡ªå®šä¹‰æ¨¡ç‰ˆã€‚å…³äºè‡ªå®šä¹‰å¯¹è¯æ¨¡æ¿ï¼Œè¯·å‚è€ƒ[è‡ªå®šä¹‰å¯¹è¯æ¨¡ç‰ˆ](https://github.com/PaddlePaddle/PaddleNLP/blob/16d3c49d2b8d0c7e56d1be8d7f6f2ca20aac80cb/docs/get_started/chat_template.md#è‡ªå®šä¹‰å¯¹è¯æ¨¡æ¿
)

ä¸ºäº†æ–¹ä¾¿æµ‹è¯•ï¼Œæˆ‘ä»¬ä¹Ÿæä¾›äº† chatml_dataset æ ¼å¼çš„æ•°æ®é›†å’Œå¯¹åº”çš„ chat_template.json,ç”¨äº qwen-vl æ¨¡å‹ç²¾è°ƒï¼Œå¯ä»¥ç›´æ¥[ä¸‹è½½](https://bj.bcebos.com/v1/paddlenlp/datasets/examples/ScienceQA.tar)ä½¿ç”¨ã€‚

**config** é…ç½®æ–‡ä»¶å‚æ•°è¯´æ˜ï¼š
```
{
    â€œmodel_name_or_pathâ€  #è®¾ç½®å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œå¦‚"qwen-vl/qwen-vl-chat-7b"

    "freeze_include"  #è®¾ç½®éœ€è¦å†»ç»“çš„å±‚ï¼Œå¦‚["*visual*"]ï¼Œé»˜è®¤None

    "freeze_exclude"  #è®¾ç½®ä¸éœ€è¦å†»ç»“çš„å±‚ï¼Œå¦‚["*visual.attn_pool*"]ï¼Œé»˜è®¤None

    "dataset": {
        "train":[{"name": "chatml_dataset", "data_files": "train.json"}],
        "eval": [{"name": "chatml_dataset", "data_files": "val.json"}]
    },  #æ•°æ®é›†é…ç½®

    â€œmixtokenâ€ : #æ˜¯å¦ä½¿ç”¨mixtokenç­–ç•¥ï¼Œé»˜è®¤False,

    "device": #è®­ç»ƒç¡¬ä»¶ï¼Œnpuã€gpu

    "output_dir":  #æ¨¡å‹å­˜å‚¨è·¯å¾„

    "overwrite_output_dir": # è¦†ç›–è¾“å‡ºç›®å½•ï¼Œé»˜è®¤False

    "per_device_train_batch_size":  #è®­ç»ƒbatchå¤§å°

    â€œgradient_accumulation_stepsâ€: #åœ¨æ‰§è¡Œbackwardæ›´æ–°è¿‡ç¨‹ä¹‹å‰ï¼Œç”¨äºç´¯ç§¯æ¢¯åº¦çš„æ›´æ–°æ­¥éª¤æ•°ã€‚å¦‚è®¾ç½®ä¸º16ï¼Œå³æ‰§è¡Œ16ä¸ªstepåï¼Œæ›´æ–°ä¸€æ¬¡å‚æ•°

    "per_device_eval_batch_size"  #è¯„ä¼°batchå¤§å°

    "eval_accumulation_steps" : è¯„ä¼°ç´¯ç§¯æ­¥æ•°

    â€œsave_strategyâ€:  #è®­ç»ƒæœŸé—´è¦é‡‡ç”¨ä¿å­˜æ¨¡å‹ç­–ç•¥ã€‚å¯é€‰æ‹©ï¼š
                    #â€œnoâ€ï¼šåœ¨è®­ç»ƒæœŸé—´ä¸è¿›è¡Œä»»ä½•ä¿å­˜ã€‚
                    #â€œepochâ€`ï¼šæ¯ä¸ªepochåä¿å­˜ã€‚
                    #â€œstepsâ€`ï¼šæ¯â€œSave_stepsâ€ä¿å­˜ä¸€æ¬¡ã€‚

    "save_steps":  #æ¯å¤šå°‘ä¸ªstepsä¿å­˜ä¸€æ¬¡æ¨¡å‹

    "save_total_limit":  #æœ€å¤šä¿å­˜å¤šå°‘ä¸ªæ¨¡å‹

    "evaluation_strategy":   #è¯„ä¼°ç­–ç•¥ã€‚å¯é€‰æ‹©ï¼š
                            #â€œnoâ€ï¼šåœ¨è®­ç»ƒæœŸé—´ä¸è¿›è¡Œä»»ä½•è¯„ä¼°ã€‚
                            #â€œepochâ€`ï¼šæ¯ä¸ªepochåè¯„ä¼°ã€‚
                            #â€œstepsâ€`ï¼šæ¯â€œeval_stepsâ€è¯„ä¼°ä¸€æ¬¡ã€‚

    "do_train":  #æ˜¯å¦è¿›è¡Œè®­ç»ƒ

    "bf16": #æ˜¯å¦ä½¿ç”¨bf16è®­ç»ƒï¼Œé»˜è®¤Falseï¼Œä»…æ”¯æŒa100

    "fp16": #æ˜¯ä½¿ç”¨fp16è®­ç»ƒï¼Œé»˜è®¤False

    â€œfp16_opt_levelâ€: #æ··åˆç²¾åº¦è®­ç»ƒç­‰çº§ï¼Œå¯é€‰O1,O2

    "learning_rate":  #å­¦ä¹ ç‡

    "adam_beta2":   #optimizerä¸­beta2å‚æ•°

    "warmup_ratio":  #å­¦ä¹ ç‡warm upæ¯”ä¾‹

    "weight_decay":  #æƒé‡è¡°å‡

    "lr_scheduler_type":  #å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼Œå¯é€‰cosineã€linear

    "logging_steps": #æ—¥å¿—æ‰“å°é—´éš”

    "max_length":  #æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤2048

    "benchmark":  #æ˜¯å¦å¼€å¯benchmarkæ¨¡å¼ï¼Œé»˜è®¤False

    "skip_memory_metrics":  #æ˜¯å¦è·³è¿‡å†…å­˜æŒ‡æ ‡ï¼Œé»˜è®¤False

    â€œloraâ€: #æ˜¯å¦ä½¿ç”¨LoRAç­–ç•¥ï¼Œé»˜è®¤False

    "lora_rank": #LoRA rank

    "lora_alpha": #LoRA alpha

    "lora_dropout": #LoRA dropout

    "lora_target_modules": #LoRA target modules,å¦‚
    [ ".*attn.c_attn.*",
        ".*attn.c_proj.*",
        ".*mlp.w1.*",
        ".*mlp.w2.*"]

    "tensor_parallel_degree":  # æ¨¡å‹å¹¶è¡Œç³»æ•°ï¼Œè®¾ç½®ä¸ºNåˆ™è¿›è¡ŒNå¡é—´æ¨¡å‹å¹¶è¡Œã€‚å¯é€‰å‚æ•°ã€‚

    "sharding_parallel_degree":  #æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥ï¼Œè¯¦æƒ…å‚è€ƒ [ã€ŠZeRO: Memory Optimizations Toward Training Trillion Parameter Modelsã€‹]ï¼ˆhttps://arxiv.org/abs/1910.02054ï¼‰å¯é€‰å‚æ•°ã€‚

    "sharding":  #æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥stageé€‰æ‹©ï¼Œç›®å‰æ”¯æŒstage1ã€stage2ã€‚å¯é€‰å‚æ•°ã€‚

    "pipeline_parallel_degree": #æµæ°´çº¿å¹¶è¡Œã€‚è¯¦æƒ…å‚è€ƒ[é£æ¡¨å¤§è¯­è¨€æ¨¡å‹å·¥å…·é“¾]ï¼ˆhttps://github.com/PaddlePaddle/PaddleNLP/blob/develop/llm/README.mdï¼‰å¯é€‰å‚æ•°ã€‚

}

```

> æ³¨ï¼šè‹¥ä¸éœ€è¦ sharding ç­–ç•¥ï¼Œåˆ™æ— éœ€æŒ‡å®štensor_parallel_degreeã€sharding_parallel_degreeã€shardingã€pipeline_parallel_degreeå‚æ•°

> æ›´å¤šå‚æ•°ï¼Œå¯å‚è€ƒ [argument](../trainer/argument.py)

**å…¨å‚ç²¾è°ƒï¼šSFT**
```bash
# å•å¡Qwen-vl SFTå¯åŠ¨å‘½ä»¤å‚è€ƒ
export FLAGS_use_cuda_managed_memory=true #è‹¥æ˜¾å­˜ä¸å¤Ÿï¼Œå¯è®¾ç½®ç¯å¢ƒå˜é‡
python paddlemix/tools/supervised_finetune.py paddlemix/config/qwen_vl/sft_argument.json

# å¤šå¡Qwen-vl SFTå¯åŠ¨å‘½ä»¤å‚è€ƒ
export FLAGS_use_cuda_managed_memory=true #è‹¥æ˜¾å­˜ä¸å¤Ÿï¼Œå¯è®¾ç½®ç¯å¢ƒå˜é‡
python -u  -m paddle.distributed.launch --gpus "0,1,2,3" paddlemix/tools/supervised_finetune.py paddlemix/config/qwen_vl/sft_argument.json

# æˆ–è€…ä½¿ç”¨ç»Ÿä¸€å¯åŠ¨è„šæœ¬
sh paddlemix/tools/train.sh paddlemix/config/qwen_vl/sft_argument.json
```

**LoRA**
```bash
# å•å¡Qwen-vl LoRAå¯åŠ¨å‘½ä»¤å‚è€ƒ
python  paddlemix/tools/supervised_finetune.py paddlemix/config/qwen_vl/lora_sft_argument.json

# å¤šå¡Qwen-vl LoRAå¯åŠ¨å‘½ä»¤å‚è€ƒ
python -u  -m paddle.distributed.launch --gpus "0,1,2,3" paddlemix/tools/supervised_finetune.py paddlemix/config/qwen_vl/lora_sft_argument.json

```

æ³¨ï¼šä½¿ç”¨loraè®­ç»ƒåï¼Œéœ€è¦åˆå¹¶loraå‚æ•°ï¼Œæˆ‘ä»¬æä¾›LoRAå‚æ•°åˆå¹¶è„šæœ¬ï¼Œå¯ä»¥å°†LoRAå‚æ•°åˆå¹¶åˆ°ä¸»å¹²æ¨¡å‹å¹¶ä¿å­˜ç›¸åº”çš„æƒé‡ã€‚å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
python paddlemix/tools/merge_lora_params.py \
--model_name_or_path qwen-vl/qwen-vl-chat-7b \
--lora_path output_qwen_vl\
--merge_model_path qwen_vl_merge
```

**NPUç¡¬ä»¶è®­ç»ƒ**

PaddleMIXæ”¯æŒåœ¨NPUç¡¬ä»¶ä¸Šè¿›è¡Œè®­ç»ƒï¼š
1. è¯·å…ˆå‚ç…§[PaddleCustomDevice](https://github.com/PaddlePaddle/PaddleCustomDevice/blob/develop/backends/npu/README_cn.md)å®‰è£…NPUç¡¬ä»¶Paddle
2. åœ¨configé…ç½®æ–‡ä»¶ä¸­å¢åŠ `device`å­—æ®µæŒ‡å®šè®¾å¤‡ï¼š
```json
{
    ...
    "model_name_or_path": "paddlemix/llava/llava-v1.5-7b",
    "device": "npu",
    "output_dir": "./checkpoints/llava_sft_ckpts",
    ...
}
```
3. å¯åŠ¨è®­ç»ƒå‰è¯·è®¾ç½®å¦‚ä¸‹ç¯å¢ƒå˜é‡ç”¨äºæ€§èƒ½åŠ é€Ÿå’Œç²¾åº¦å¯¹é½
```shell
export FLAGS_use_stride_kernel=0
export FLAGS_npu_storage_format=0 # å…³é—­ç§æœ‰æ ¼å¼
export FLAGS_npu_jit_compile=0 # å…³é—­å³æ—¶ç¼–è¯‘
export FLAGS_npu_scale_aclnn=True # aclnnåŠ é€Ÿ
export FLAGS_npu_split_aclnn=True # aclnnåŠ é€Ÿ
export CUSTOM_DEVICE_BLACK_LIST=set_value,set_value_with_tensor # set_valueåŠ å…¥é»‘åå•
```
ç›®å‰æ”¯æŒNPUè®­ç»ƒçš„æ¨¡å‹å¯ä»¥å‚è€ƒæ­¤[æ–‡æ¡£](../examples/README.md)