## ğŸ“¦ PaddleMIXå·¥å…·ç®±ä»‹ç» ğŸ“¦
PaddleMIXå·¥å…·ç®±ç§‰æ‰¿äº†é£æ¡¨å¥—ä»¶ä¸€ç«™å¼ä½“éªŒã€æ€§èƒ½æè‡´ã€ç”Ÿæ€å…¼å®¹çš„è®¾è®¡ç†å¿µï¼Œæ—¨åœ¨æä¾›ä¸šç•Œä¸»æµè·¨æ¨¡æ€å¤§æ¨¡å‹å…¨æµç¨‹ç»Ÿä¸€å·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…ä½æˆæœ¬ã€ä½é—¨æ§›ã€å¿«é€Ÿå®ç°è·¨æ¨¡æ€å¤§æ¨¡å‹å®šåˆ¶åŒ–ã€‚


##  ğŸ› ï¸ æ”¯æŒæ¨¡å‹åˆ—è¡¨ ğŸ› ï¸
| Model | Inference |Pretrain | SFT | LoRA | Deploy |
| --- | --- | --- | --- | --- | --- |
| [qwen_vl](../examples/qwen_vl/) | âœ…  | âŒ  | âœ…  | âœ…  | ğŸš§  |
| [blip2](../examples/blip2/) | âœ…  | âœ… | âœ…  | âŒ | âœ…  |
| [visualglm](../examples/visualglm/) | âœ… | âŒ |ğŸš§ | ğŸš§ | âŒ |

* âœ…: Supported
* ğŸš§: In Progress
* âŒ: Not Supported

æ³¨æ„ï¼š
1. å¼€å§‹å‰è¯·å…ˆæŒ‰ç…§[ç¯å¢ƒä¾èµ–](../../README.md#ç¯å¢ƒä¾èµ–)å®‰è£…ç¯å¢ƒï¼Œä¸åŒæ¨¡å‹è¯·å‚è€ƒ [examples](../examples/README.md) ä¸‹å¯¹åº”çš„æ¨¡å‹ç›®å½•å®‰è£…ä¾èµ–ï¼›
2. å½“å‰**tools**ç»Ÿä¸€æ¥å£åªæ”¯æŒéƒ¨åˆ†æ¨¡å‹çš„ç²¾è°ƒèƒ½åŠ›ï¼Œå…¶ä»–æ¨¡å‹åŠå…¶ä»–èƒ½åŠ›åç»­é™†ç»­ä¸Šçº¿ã€‚


##  ğŸš€ å¿«é€Ÿå¼€å§‹ ğŸš€

### 1. ç²¾è°ƒ
PaddleMIX ç²¾è°ƒæ”¯æŒå¤šä¸ªä¸»æµè·¨æ¨¡æ€å¤§æ¨¡å‹çš„SFTã€LoRAç­‰ç²¾è°ƒç­–ç•¥ï¼Œæä¾›ç»Ÿä¸€ã€é«˜æ•ˆç²¾è°ƒæ–¹æ¡ˆï¼š
- **ç»Ÿä¸€è®­ç»ƒå…¥å£** PaddleMIX ç²¾è°ƒæ–¹æ¡ˆå¯é€‚é…ä¸šç•Œä¸»æµè·¨æ¨¡æ€å¤§æ¨¡å‹ï¼Œç”¨æˆ·åªéœ€ä¿®æ”¹[config](../config/) é…ç½®æ–‡ä»¶ï¼Œå³èƒ½åœ¨å•å¡æˆ–å¤šå¡è¿›è¡Œå¤šç§å¤§æ¨¡å‹ç²¾è°ƒï¼›
- **å¤šæ•°æ®é›†å’Œæ··åˆæ•°æ®é›†** æ”¯æŒå¤šç§æ•°æ®é›†å’Œæ··åˆæ•°æ®é›†åŒæ—¶ç²¾è°ƒï¼ŒåŒ…æ‹¬ï¼šVQAã€Captionã€Chatmlç­‰æ•°æ®é›†æ··åˆä½¿ç”¨ï¼›
- **å¼ºå¤§æ•°æ®æµå’Œåˆ†å¸ƒå¼ç­–ç•¥** MIXTokenç­–ç•¥æœ‰æ•ˆå¢åŠ æ•°æ®ååé‡ï¼Œå¤§å¹…åº¦æé«˜æ¨¡å‹è®­ç»ƒæ•ˆç‡ã€‚è‡ªé€‚åº”Trainerå’Œå®šåˆ¶åŒ–Trainerçµæ´»é…ç½®ï¼Œæ— ç¼é“¾æ¥é£æ¡¨åˆ†å¸ƒå¼å¹¶è¡Œç­–ç•¥ï¼Œå¤§å¹…é™ä½å¤§æ¨¡å‹ç²¾è°ƒç¡¬ä»¶é—¨æ§›ã€‚


**æ•°æ®å‡†å¤‡**ï¼š

æˆ‘ä»¬æ”¯æŒå¤šæ•°æ®é›†ã€æ··åˆæ•°æ®é›†åŒæ—¶ç”¨äºç²¾è°ƒï¼Œé€šè¿‡**MixDataset**ç»Ÿä¸€è°ƒåº¦ï¼Œç”¨æˆ·åªéœ€åœ¨é…ç½®æ–‡ä»¶æŒ‡å®š [dataset](../datasets/) æ”¯æŒçš„æ•°æ®é›†ç»„æˆé›†åˆï¼Œå³å¯ä½¿ç”¨ã€‚å¦‚ï¼š

```
# config.json
...

"dataset": {
      "train":[
        {"name": "chatml_dataset", "data_files": "train.json"},
        {"name": "coco_caption", "data_files": "train.json"},
        ......],
      "eval": [
        {"name": "chatml_dataset", "data_files": "val.json"},
        {"name": "coco_caption", "data_files": "val.json"},
        ......],
    },
....

```

è€Œå¯¹äºæ¯ä¸ªå­æ•°æ®é›†ï¼Œå¦‚ä¸Šè¿°çš„ chatml_datasetã€coco_caption æ ¼å¼ï¼Œå¯å‚è€ƒ[examples](../examples/) ä¸‹å¯¹åº”çš„æ¨¡å‹ç›®å½•é‡Œé¢çš„æ–‡æ¡£ä»‹ç»ã€‚

ä¸ºäº†æ–¹ä¾¿æµ‹è¯•ï¼Œæˆ‘ä»¬ä¹Ÿæä¾›äº†chatml_datasetæ ¼å¼çš„æ•°æ®é›†,ç”¨äº qwen-vl æ¨¡å‹ç²¾è°ƒï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼š
```bash
wget https://bj.bcebos.com/v1/paddlenlp/datasets/examples/ocrvqa_examples.zip
unzip ocrvqa_examples.zip
```

**config** é…ç½®æ–‡ä»¶å‚æ•°è¯´æ˜ï¼š
```
{
    â€œmodel_name_or_pathâ€  #è®¾ç½®å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œå¦‚"qwen-vl/qwen-vl-chat-7b"

    "freeze_include"  #è®¾ç½®éœ€è¦å†»ç»“çš„å±‚ï¼Œå¦‚["*visual*"]ï¼Œé»˜è®¤None

    "freeze_exclude"  #è®¾ç½®ä¸éœ€è¦å†»ç»“çš„å±‚ï¼Œå¦‚["*visual.attn_pool*"]ï¼Œé»˜è®¤None

    "dataset": {
        "train":[{"name": "chatml_dataset", "data_files": "train.json"}],
        "eval": [{"name": "chatml_dataset", "data_files": "val.json"}]
    },  #æ•°æ®é›†é…ç½®

    â€œmixtokenâ€ : #æ˜¯å¦ä½¿ç”¨mixtokenç­–ç•¥ï¼Œé»˜è®¤False,

    "output_dir":  #æ¨¡å‹å­˜å‚¨è·¯å¾„

    "overwrite_output_dir": # è¦†ç›–è¾“å‡ºç›®å½•ï¼Œé»˜è®¤False

    "per_device_train_batch_size":  #è®­ç»ƒbatchå¤§å°

    â€œgradient_accumulation_stepsâ€: #åœ¨æ‰§è¡Œbackwardæ›´æ–°è¿‡ç¨‹ä¹‹å‰ï¼Œç”¨äºç´¯ç§¯æ¢¯åº¦çš„æ›´æ–°æ­¥éª¤æ•°ã€‚å¦‚è®¾ç½®ä¸º16ï¼Œå³æ‰§è¡Œ16ä¸ªstepåï¼Œæ›´æ–°ä¸€æ¬¡å‚æ•°

    "per_device_eval_batch_size"  #è¯„ä¼°batchå¤§å°

    "eval_accumulation_steps" : è¯„ä¼°ç´¯ç§¯æ­¥æ•°

    â€œsave_strategyâ€:  #è®­ç»ƒæœŸé—´è¦é‡‡ç”¨ä¿å­˜æ¨¡å‹ç­–ç•¥ã€‚å¯é€‰æ‹©ï¼š
                    #â€œnoâ€ï¼šåœ¨è®­ç»ƒæœŸé—´ä¸è¿›è¡Œä»»ä½•ä¿å­˜ã€‚
                    #â€œepochâ€`ï¼šæ¯ä¸ªepochåä¿å­˜ã€‚
                    #â€œstepsâ€`ï¼šæ¯â€œSave_stepsâ€ä¿å­˜ä¸€æ¬¡ã€‚

    "save_steps":  #æ¯å¤šå°‘ä¸ªstepsä¿å­˜ä¸€æ¬¡æ¨¡å‹

    "save_total_limit":  #æœ€å¤šä¿å­˜å¤šå°‘ä¸ªæ¨¡å‹

    "evaluation_strategy":   #è¯„ä¼°ç­–ç•¥ã€‚å¯é€‰æ‹©ï¼š
                            #â€œnoâ€ï¼šåœ¨è®­ç»ƒæœŸé—´ä¸è¿›è¡Œä»»ä½•è¯„ä¼°ã€‚
                            #â€œepochâ€`ï¼šæ¯ä¸ªepochåè¯„ä¼°ã€‚
                            #â€œstepsâ€`ï¼šæ¯â€œeval_stepsâ€è¯„ä¼°ä¸€æ¬¡ã€‚

    "do_train":  #æ˜¯å¦è¿›è¡Œè®­ç»ƒ

    "bf16": #æ˜¯å¦ä½¿ç”¨bf16è®­ç»ƒï¼Œé»˜è®¤Falseï¼Œä»…æ”¯æŒa100

    "fp16": #æ˜¯ä½¿ç”¨fp16è®­ç»ƒï¼Œé»˜è®¤False

    â€œfp16_opt_levelâ€: #æ··åˆç²¾åº¦è®­ç»ƒç­‰çº§ï¼Œå¯é€‰O1,O2

    "learning_rate":  #å­¦ä¹ ç‡

    "adam_beta2":   #optimizerä¸­beta2å‚æ•°

    "warmup_ratio":  #å­¦ä¹ ç‡warm upæ¯”ä¾‹

    "weight_decay":  #æƒé‡è¡°å‡

    "lr_scheduler_type":  #å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼Œå¯é€‰cosineã€linear

    "logging_steps": #æ—¥å¿—æ‰“å°é—´éš”

    "max_length":  #æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤2048

    "benchmark":  #æ˜¯å¦å¼€å¯benchmarkæ¨¡å¼ï¼Œé»˜è®¤False

    "skip_memory_metrics":  #æ˜¯å¦è·³è¿‡å†…å­˜æŒ‡æ ‡ï¼Œé»˜è®¤False

    â€œloraâ€: #æ˜¯å¦ä½¿ç”¨LoRAç­–ç•¥ï¼Œé»˜è®¤False

    "lora_rank": #LoRA rank

    "lora_alpha": #LoRA alpha

    "lora_dropout": #LoRA dropout

    "lora_target_modules": #LoRA target modules,å¦‚
    [ ".*attn.c_attn.*",
        ".*attn.c_proj.*",
        ".*mlp.w1.*",
        ".*mlp.w2.*"]

    "tensor_parallel_degree":  # æ¨¡å‹å¹¶è¡Œç³»æ•°ï¼Œè®¾ç½®ä¸ºNåˆ™è¿›è¡ŒNå¡é—´æ¨¡å‹å¹¶è¡Œã€‚å¯é€‰å‚æ•°ã€‚

    "sharding_parallel_degree":  #æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥ï¼Œè¯¦æƒ…å‚è€ƒ [ã€ŠZeRO: Memory Optimizations Toward Training Trillion Parameter Modelsã€‹]ï¼ˆhttps://arxiv.org/abs/1910.02054ï¼‰å¯é€‰å‚æ•°ã€‚

    "sharding":  #æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥stageé€‰æ‹©ï¼Œç›®å‰æ”¯æŒstage1ã€stage2ã€‚å¯é€‰å‚æ•°ã€‚

    "pipeline_parallel_degree": #æµæ°´çº¿å¹¶è¡Œã€‚è¯¦æƒ…å‚è€ƒ[é£æ¡¨å¤§è¯­è¨€æ¨¡å‹å·¥å…·é“¾]ï¼ˆhttps://github.com/PaddlePaddle/PaddleNLP/blob/develop/llm/README.mdï¼‰å¯é€‰å‚æ•°ã€‚

}

```

> æ³¨ï¼šè‹¥ä¸éœ€è¦ sharding ç­–ç•¥ï¼Œåˆ™æ— éœ€æŒ‡å®štensor_parallel_degreeã€sharding_parallel_degreeã€shardingã€pipeline_parallel_degreeå‚æ•°

> æ›´å¤šå‚æ•°ï¼Œå¯å‚è€ƒ [argument](../trainer/argument.py)

**å…¨å‚ç²¾è°ƒï¼šSFT**
```bash
# å•å¡Qwen-vl SFTå¯åŠ¨å‘½ä»¤å‚è€ƒ
python paddlemix/tools/supervised_finetune.py paddlemix/config/qwen_vl/sft_argument.json

# å¤šå¡Qwen-vl SFTå¯åŠ¨å‘½ä»¤å‚è€ƒ
python -u  -m paddle.distributed.launch --gpus "0,1,2,3" paddlemix/tools/supervised_finetune.py paddlemix/config/qwen_vl/sft_argument.json
```

**LoRA**
```bash
# å•å¡Qwen-vl LoRAå¯åŠ¨å‘½ä»¤å‚è€ƒ
python  paddlemix/tools/supervised_finetune.py paddlemix/config/qwen_vl/lora_sft_argument.json
```

æ³¨ï¼šä½¿ç”¨loraè®­ç»ƒåï¼Œéœ€è¦åˆå¹¶loraå‚æ•°ï¼Œæˆ‘ä»¬æä¾›LoRAå‚æ•°åˆå¹¶è„šæœ¬ï¼Œå¯ä»¥å°†LoRAå‚æ•°åˆå¹¶åˆ°ä¸»å¹²æ¨¡å‹å¹¶ä¿å­˜ç›¸åº”çš„æƒé‡ã€‚å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
python paddlemix/paddlemix/tools/merge_lora_params.py \
--model_name_or_path qwen-vl/qwen-vl-chat-7b \
--lora_path output_qwen_vl\
--merge_model_path qwen_vl_merge
```
