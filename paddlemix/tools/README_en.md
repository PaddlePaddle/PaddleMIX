## ğŸ“¦ PaddleMIX Tools IntroductionğŸ“¦
The PaddleMIX toolkit embodies the design philosophy of one-stop experience, ultimate performance, and ecosystem compatibility upheld by the PaddlePaddle suite. It aims to provide developers with a unified set of tools for the entire process of cross-modal large-model development, aligning with industry standards. This facilitates low-cost, low-threshold, and rapid customization of cross-modal large models.

[[ä¸­æ–‡æ–‡æ¡£](README.md)]

##  ğŸ› ï¸ Supported Model List ğŸ› ï¸
| Model | Inference |Pretrain | SFT | LoRA | Deploy |
| --- | --- | --- | --- | --- | --- |
| [qwen_vl](../examples/qwen_vl/) | âœ…  | âŒ  | âœ…  | âœ…  |  âœ… |
| [blip2](../examples/blip2/) | âœ…  | âœ… | âœ…  | âœ… | âœ…  |
| [visualglm](../examples/visualglm/) | âœ… | âŒ | âœ… | âœ… | âŒ |
| [llava](../examples/llava/) | âœ…  | âœ…   | âœ…  | âœ…  | ğŸš§  |

* âœ…: Supported
* ğŸš§: In Progress
* âŒ: Not Supported

Note:
1. å¼€å§‹å‰è¯·å…ˆæŒ‰ç…§[ç¯å¢ƒä¾èµ–](../../README_EN.md#environment-dependencies)å®‰è£…ç¯å¢ƒï¼Œä¸åŒæ¨¡å‹è¯·å‚è€ƒ [examples](../examples/README.md) ä¸‹å¯¹åº”çš„æ¨¡å‹ç›®å½•å®‰è£…ä¾èµ–ï¼›
2. å½“å‰**tools**ç»Ÿä¸€æ¥å£åªæ”¯æŒéƒ¨åˆ†æ¨¡å‹çš„ç²¾è°ƒèƒ½åŠ›ï¼Œå…¶ä»–æ¨¡å‹åŠå…¶ä»–èƒ½åŠ›åç»­é™†ç»­ä¸Šçº¿ã€‚


##  ğŸš€ Quick Start ğŸš€

### 1. Fine-tuning
PaddleMIX fine-tuning supports various mainstream large multi-modal fine-tuning strategies such as SFT and LoRA, providing a unified and efficient fine-tuning solution:
- **Unified Training Entry**: The PaddleMIX fine-tuning solution is adaptable to various mainstream large multi-modal models. Users only need to modify the configuration file in [config](../config/) to perform fine-tuning on single or multiple GPUs for different large models.
- **Multiple and Mixed Datasets**: Supports fine-tuning with multiple datasets and mixed datasets simultaneously, including a mixture of datasets such as VQA, Caption, Chatml, etc.
- **Powerful Data Flow and Distributed Strategies**: The MIXToken strategy effectively increases data throughput, significantly improving model training efficiency. Adaptive Trainer and customizable Trainer configurations seamlessly integrate with Paddle's distributed parallelism strategies, greatly reducing the hardware threshold for fine-tuning large models.

**Data Preparation**ï¼š

We support the use of multiple datasets and mixed datasets simultaneously for fine-tuning. This is achieved through **MixDataset** scheduling. Users only need to specify in the configuration file a collection of datasets supported by [dataset](../datasets/). For example:
```
# config.json
...

"dataset": {
      "train":[
        {"name": "chatml_dataset", "data_files": "train.json"ï¼Œ"chat_template":"chat_template.json"},
        {"name": "coco_caption", "data_files": "train.json"},
        ......],
      "eval": [
        {"name": "chatml_dataset", "data_files": "val.json"ï¼Œ"chat_template":"chat_template.json"},
        {"name": "coco_caption", "data_files": "val.json"},
        ......],
    },
....

```

For each sub-dataset, such as the coco_caption dataset mentioned above, you can refer to the documentation inside the corresponding model directory under [examples](../examples/).

Additionally, we support mainstream LLM dialogue custom templates through the chatml_dataset dataset. For information on customizing dialogue templates, please refer to [Custom Dialogue Templates](https://github.com/PaddlePaddle/PaddleNLP/blob/16d3c49d2b8d0c7e56d1be8d7f6f2ca20aac80cb/docs/get_started/chat_template.md#è‡ªå®šä¹‰å¯¹è¯æ¨¡æ¿).

For convenience in testing, we also provide a dataset in the chatml_dataset format and the corresponding chat_template.json file for fine-tuning the qwen-vl model. You can directly [download](https://bj.bcebos.com/v1/paddlenlp/datasets/examples/ScienceQA.tar) and use it.

**config** Configuration File Parameter Explanationï¼š
```
{
    â€œmodel_name_or_pathâ€  #è®¾ç½®å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œå¦‚"qwen-vl/qwen-vl-chat-7b"

    "freeze_include"  #è®¾ç½®éœ€è¦å†»ç»“çš„å±‚ï¼Œå¦‚["*visual*"]ï¼Œé»˜è®¤None

    "freeze_exclude"  #è®¾ç½®ä¸éœ€è¦å†»ç»“çš„å±‚ï¼Œå¦‚["*visual.attn_pool*"]ï¼Œé»˜è®¤None

    "dataset": {
        "train":[{"name": "chatml_dataset", "data_files": "train.json"}],
        "eval": [{"name": "chatml_dataset", "data_files": "val.json"}]
    },  #æ•°æ®é›†é…ç½®

    â€œmixtokenâ€ : #æ˜¯å¦ä½¿ç”¨mixtokenç­–ç•¥ï¼Œé»˜è®¤False,

    "output_dir":  #æ¨¡å‹å­˜å‚¨è·¯å¾„

    "overwrite_output_dir": # è¦†ç›–è¾“å‡ºç›®å½•ï¼Œé»˜è®¤False

    "per_device_train_batch_size":  #è®­ç»ƒbatchå¤§å°

    â€œgradient_accumulation_stepsâ€: #åœ¨æ‰§è¡Œbackwardæ›´æ–°è¿‡ç¨‹ä¹‹å‰ï¼Œç”¨äºç´¯ç§¯æ¢¯åº¦çš„æ›´æ–°æ­¥éª¤æ•°ã€‚å¦‚è®¾ç½®ä¸º16ï¼Œå³æ‰§è¡Œ16ä¸ªstepåï¼Œæ›´æ–°ä¸€æ¬¡å‚æ•°

    "per_device_eval_batch_size"  #è¯„ä¼°batchå¤§å°

    "eval_accumulation_steps" : è¯„ä¼°ç´¯ç§¯æ­¥æ•°

    â€œsave_strategyâ€:  #è®­ç»ƒæœŸé—´è¦é‡‡ç”¨ä¿å­˜æ¨¡å‹ç­–ç•¥ã€‚å¯é€‰æ‹©ï¼š
                    #â€œnoâ€ï¼šåœ¨è®­ç»ƒæœŸé—´ä¸è¿›è¡Œä»»ä½•ä¿å­˜ã€‚
                    #â€œepochâ€`ï¼šæ¯ä¸ªepochåä¿å­˜ã€‚
                    #â€œstepsâ€`ï¼šæ¯â€œSave_stepsâ€ä¿å­˜ä¸€æ¬¡ã€‚

    "save_steps":  #æ¯å¤šå°‘ä¸ªstepsä¿å­˜ä¸€æ¬¡æ¨¡å‹

    "save_total_limit":  #æœ€å¤šä¿å­˜å¤šå°‘ä¸ªæ¨¡å‹

    "evaluation_strategy":   #è¯„ä¼°ç­–ç•¥ã€‚å¯é€‰æ‹©ï¼š
                            #â€œnoâ€ï¼šåœ¨è®­ç»ƒæœŸé—´ä¸è¿›è¡Œä»»ä½•è¯„ä¼°ã€‚
                            #â€œepochâ€`ï¼šæ¯ä¸ªepochåè¯„ä¼°ã€‚
                            #â€œstepsâ€`ï¼šæ¯â€œeval_stepsâ€è¯„ä¼°ä¸€æ¬¡ã€‚

    "do_train":  #æ˜¯å¦è¿›è¡Œè®­ç»ƒ

    "bf16": #æ˜¯å¦ä½¿ç”¨bf16è®­ç»ƒï¼Œé»˜è®¤Falseï¼Œä»…æ”¯æŒa100

    "fp16": #æ˜¯ä½¿ç”¨fp16è®­ç»ƒï¼Œé»˜è®¤False

    â€œfp16_opt_levelâ€: #æ··åˆç²¾åº¦è®­ç»ƒç­‰çº§ï¼Œå¯é€‰O1,O2

    "learning_rate":  #å­¦ä¹ ç‡

    "adam_beta2":   #optimizerä¸­beta2å‚æ•°

    "warmup_ratio":  #å­¦ä¹ ç‡warm upæ¯”ä¾‹

    "weight_decay":  #æƒé‡è¡°å‡

    "lr_scheduler_type":  #å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼Œå¯é€‰cosineã€linear

    "logging_steps": #æ—¥å¿—æ‰“å°é—´éš”

    "max_length":  #æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤2048

    "benchmark":  #æ˜¯å¦å¼€å¯benchmarkæ¨¡å¼ï¼Œé»˜è®¤False

    "skip_memory_metrics":  #æ˜¯å¦è·³è¿‡å†…å­˜æŒ‡æ ‡ï¼Œé»˜è®¤False

    â€œloraâ€: #æ˜¯å¦ä½¿ç”¨LoRAç­–ç•¥ï¼Œé»˜è®¤False

    "lora_rank": #LoRA rank

    "lora_alpha": #LoRA alpha

    "lora_dropout": #LoRA dropout

    "lora_target_modules": #LoRA target modules,å¦‚
    [ ".*attn.c_attn.*",
        ".*attn.c_proj.*",
        ".*mlp.w1.*",
        ".*mlp.w2.*"]

    "tensor_parallel_degree":  # æ¨¡å‹å¹¶è¡Œç³»æ•°ï¼Œè®¾ç½®ä¸ºNåˆ™è¿›è¡ŒNå¡é—´æ¨¡å‹å¹¶è¡Œã€‚å¯é€‰å‚æ•°ã€‚

    "sharding_parallel_degree":  #æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥ï¼Œè¯¦æƒ…å‚è€ƒ [ã€ŠZeRO: Memory Optimizations Toward Training Trillion Parameter Modelsã€‹]ï¼ˆhttps://arxiv.org/abs/1910.02054ï¼‰å¯é€‰å‚æ•°ã€‚

    "sharding":  #æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥stageé€‰æ‹©ï¼Œç›®å‰æ”¯æŒstage1ã€stage2ã€‚å¯é€‰å‚æ•°ã€‚

    "pipeline_parallel_degree": #æµæ°´çº¿å¹¶è¡Œã€‚è¯¦æƒ…å‚è€ƒ[é£æ¡¨å¤§è¯­è¨€æ¨¡å‹å·¥å…·é“¾]ï¼ˆhttps://github.com/PaddlePaddle/PaddleNLP/blob/develop/llm/README.mdï¼‰å¯é€‰å‚æ•°ã€‚

}

```

> Note: If you do not need the sharding strategy, you do not need to specify the tensor_parallel_degree, sharding_parallel_degree, sharding, or pipeline_parallel_degree parameters.

> For more parameters, please refer to [argument](../trainer/argument.py).

**Full-parameter Fine-tuning: SFT**
```bash
# å•å¡Qwen-vl SFTå¯åŠ¨å‘½ä»¤å‚è€ƒ
python paddlemix/tools/supervised_finetune.py paddlemix/config/qwen_vl/sft_argument.json

# å¤šå¡Qwen-vl SFTå¯åŠ¨å‘½ä»¤å‚è€ƒ
python -u  -m paddle.distributed.launch --gpus "0,1,2,3" paddlemix/tools/supervised_finetune.py paddlemix/config/qwen_vl/sft_argument.json
```

**LoRA**
```bash
# å•å¡Qwen-vl LoRAå¯åŠ¨å‘½ä»¤å‚è€ƒ
python  paddlemix/tools/supervised_finetune.py paddlemix/config/qwen_vl/lora_sft_argument.json
```

Note: After training with LoRA, it's necessary to merge the LoRA parameters. We provide a script for merging LoRA parameters, which combines the LoRA parameters into the main model and saves the corresponding weights. The command is as follows:

```bash
python paddlemix/paddlemix/tools/merge_lora_params.py \
--model_name_or_path qwen-vl/qwen-vl-chat-7b \
--lora_path output_qwen_vl\
--merge_model_path qwen_vl_merge
```
